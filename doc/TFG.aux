\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{TFG.ist}
\@glsorder{word}
\babel@aux{spanish}{}
\citation{zhang2023dive}
\citation{Krizhevsky2012ImageNetCW}
\citation{googledev-rocauc}
\citation{examples_nn__kulik_2009}
\citation{nn_dna_sequences__snyder_1993}
\citation{machine_translation__mahata_2019}
\citation{chatbot_customer_service__nuruzzaman_2018}
\citation{conv_nn_face_recog__chowanda_2019}
\citation{slam_vehicles__saleem_2023}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introducción}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:introduccion}{{1}{1}{Introducción}{chapter.1}{}}
\citation{rna_fundamentos__hilera_2021}
\citation{nn_fundamentals__thakur_2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Redes Neuronales Artificiales}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:red_neuronal_tradicional}{{2}{2}{Redes Neuronales Artificiales}{chapter.2}{}}
\citation{yolo_docs__2024}
\citation{neurocomputing__hecht_nielsen_1998}
\citation{intro_rna__rivera_2005}
\citation{neurocomputing__hecht_nielsen_1998}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Arquitectura de una Red Neuronal}{3}{section.2.1}\protected@file@percent }
\newlabel{sec:arquitectura_rna}{{2.1}{3}{Arquitectura de una Red Neuronal}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Red neuronal totalmente conectada (3–4–2).}}{3}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:red_neuronal_simple}{{2.1}{3}{Red neuronal totalmente conectada (3–4–2)}{figure.caption.5}{}}
\citation{rna_fundamentos__hilera_2021}
\citation{intro_rna__rivera_2005}
\citation{dl_python__chollet_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Las capas}{4}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}La neurona}{4}{subsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Diagrama funcionamiento neurona simple.}}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:diagrama_neurona_simple}{{2.2}{4}{Diagrama funcionamiento neurona simple}{figure.caption.6}{}}
\citation{analisis_matematico__gordon_1995}
\citation{dl_python__chollet_2021}
\citation{dl_python__chollet_2021,dl_fundamentos__casas_roma_2020}
\citation{nn_dl__michael_nielsen_2015}
\citation{nn_dl__michael_nielsen_2015}
\@writefile{toc}{\contentsline {subsubsection}{Función de Combinación}{5}{subsubsection*.7}\protected@file@percent }
\newlabel{f_combinacion}{{2.1.2}{5}{Función de Combinación}{subsubsection*.7}{}}
\newlabel{f_combinacion:suma_ponderada}{{2.1}{5}{Función de Combinación}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Función de Activación}{5}{subsubsection*.8}\protected@file@percent }
\newlabel{f_activacion}{{2.1.2}{5}{Función de Activación}{subsubsection*.8}{}}
\newlabel{f_activacion:umbral}{{2.2}{5}{Función de Activación}{equation.2.2}{}}
\newlabel{f_activacion:sesgo}{{2.3}{5}{Función de Activación}{equation.2.3}{}}
\citation{nn_dl__michael_nielsen_2015}
\citation{rna_fundamentos__hilera_2021}
\citation{perceptron__rosenblatt_1958}
\citation{nn_dl__michael_nielsen_2015}
\citation{nn_dl__michael_nielsen_2015}
\citation{nn_dl__michael_nielsen_2015}
\newlabel{f_combinacion:sesgo}{{2.4}{6}{Función de Activación}{equation.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Perceptrón}{6}{section*.9}\protected@file@percent }
\newlabel{f_activacion:escalon}{{2.5}{6}{Perceptrón}{equation.2.5}{}}
\citation{nn_dl__michael_nielsen_2015}
\citation{dl_fundamentos__casas_roma_2020}
\citation{dl_python__chollet_2021}
\citation{dl_python__chollet_2021}
\@writefile{toc}{\contentsline {paragraph}{Sigmoide}{7}{section*.10}\protected@file@percent }
\newlabel{f_activacion:sigmoide}{{2.6}{7}{Sigmoide}{equation.2.6}{}}
\@writefile{toc}{\contentsline {paragraph}{ReLU}{7}{section*.11}\protected@file@percent }
\newlabel{sec:relu}{{2.1.2}{7}{ReLU}{section*.11}{}}
\newlabel{f_activacion:relu}{{2.7}{7}{ReLU}{equation.2.7}{}}
\citation{dl__goodfellow_2016}
\citation{dl_fundamentos__casas_roma_2020}
\citation{dl_python__chollet_2021,dl_fundamentos__casas_roma_2020}
\citation{dl_python__chollet_2021,dl_fundamentos__casas_roma_2020}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Entrenamiento}{8}{section.2.2}\protected@file@percent }
\newlabel{sec:entrenamiento_rn}{{2.2}{8}{Entrenamiento}{section.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Preparación de los Datos}{8}{section*.12}\protected@file@percent }
\citation{dl_fundamentos__casas_roma_2020}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Preparación del conjunto de datos MNIST en los subconjuntos de entrenamiento, validación y pruebas.}}{9}{figure.caption.13}\protected@file@percent }
\newlabel{fig:preparcion_datos_mnist}{{2.3}{9}{Preparación del conjunto de datos MNIST en los subconjuntos de entrenamiento, validación y pruebas}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Esquema de \emph  {K-fold Cross Validation} con \(K=5\): en cada iteración, un fold distinto (naranja) se usa para validación y los restantes (verde) para entrenamiento.}}{9}{figure.caption.14}\protected@file@percent }
\newlabel{fig:kfold_cross_validation_ejemplo}{{2.4}{9}{Esquema de \emph {K-fold Cross Validation} con \(K=5\): en cada iteración, un fold distinto (naranja) se usa para validación y los restantes (verde) para entrenamiento}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Retropropagación}{10}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:retropropagacion}{{2.2.1}{10}{Retropropagación}{subsection.2.2.1}{}}
\newlabel{f_error:cuadratico}{{2.8}{10}{Retropropagación}{equation.2.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Descenso del Gradiente}{10}{section*.15}\protected@file@percent }
\newlabel{sec:descenso_gradiente}{{2.2.1}{10}{Descenso del Gradiente}{section*.15}{}}
\newlabel{code:descenso_grad_2d_1}{{2.2.1}{10}{}{lstlisting.2.-1}{}}
\citation{dl_python__chollet_2021}
\newlabel{code:descenso_grad_2d_2}{{2.2.1}{11}{}{lstlisting.2.-2}{}}
\newlabel{code:descenso_grad_2d_3}{{2.2.1}{11}{}{lstlisting.2.-3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Descenso del gradiente para el error cuadrático. Cada paso acerca $x$ al mínimo global ($x=0$).}}{11}{figure.caption.16}\protected@file@percent }
\newlabel{fig:descenso_gradiente_2d}{{2.5}{11}{Descenso del gradiente para el error cuadrático. Cada paso acerca $x$ al mínimo global ($x=0$)}{figure.caption.16}{}}
\citation{dl__goodfellow_2016}
\newlabel{code:descenso_grad_3d_1}{{2.2.1}{12}{}{lstlisting.2.-4}{}}
\citation{dl_python__chollet_2021,dl_fundamentos__casas_roma_2020,dl__goodfellow_2016}
\citation{dl_python__chollet_2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Descenso del gradiente en 3D del error cuadrático. Cada paso acerca $(x,y)$ al mínimo global $f(x,y) = 0$.}}{13}{figure.caption.17}\protected@file@percent }
\newlabel{fig:descenso_gradiente_3d}{{2.6}{13}{Descenso del gradiente en 3D del error cuadrático. Cada paso acerca $(x,y)$ al mínimo global $f(x,y) = 0$}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Actualización de los Parámetros}{13}{section*.18}\protected@file@percent }
\newlabel{sec:actualizacion_parametros}{{2.2.1}{13}{Actualización de los Parámetros}{section*.18}{}}
\newlabel{regla_delta}{{2.9}{13}{Actualización de los Parámetros}{equation.2.9}{}}
\@writefile{toc}{\contentsline {paragraph}{La Regla de la Cadena}{14}{section*.19}\protected@file@percent }
\newlabel{sec:regla_cadena}{{2.2.1}{14}{La Regla de la Cadena}{section*.19}{}}
\citation{dl__goodfellow_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Diagrama de la función $f(x,y,z)=(x+y)z$, para $(x,y,z)=(2,3,4)$.}}{15}{figure.caption.20}\protected@file@percent }
\newlabel{fig:forward_pass_diagrama}{{2.7}{15}{Diagrama de la función $f(x,y,z)=(x+y)z$, para $(x,y,z)=(2,3,4)$}{figure.caption.20}{}}
\newlabel{fig:backward_diagrama}{{\caption@xref {fig:backward_diagrama}{ on input line 392}}{15}{La Regla de la Cadena}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Diagrama de regla de la cadena de la función $f(x,y,z)=(x+y)z$, para $(x,y,z)=(2,3,1)$.}}{15}{figure.caption.21}\protected@file@percent }
\citation{dl__goodfellow_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Generalización y Regularización}{16}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Problemas comunes en el entrenamiento}{16}{section*.22}\protected@file@percent }
\newlabel{eq:l1_reg}{{2.11}{16}{Problemas comunes en el entrenamiento}{equation.2.11}{}}
\citation{dl_python__chollet_2021}
\citation{dl__goodfellow_2016}
\newlabel{eq:l2_reg}{{2.12}{17}{Problemas comunes en el entrenamiento}{equation.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Ilustración del dropout: algunas neuronas (en gris con aspa) se desactivan de forma aleatoria durante el entrenamiento, forzando a la red a no depender de unidades específicas.}}{17}{figure.caption.23}\protected@file@percent }
\newlabel{fig:dropout}{{2.9}{17}{Ilustración del dropout: algunas neuronas (en gris con aspa) se desactivan de forma aleatoria durante el entrenamiento, forzando a la red a no depender de unidades específicas}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Limitaciones de las Redes Neuronales Tradicionales}{18}{section.2.3}\protected@file@percent }
\newlabel{sec:limitaciones_rn}{{2.3}{18}{Limitaciones de las Redes Neuronales Tradicionales}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Aplanado de una imagen para servir de entrada a una red neuronal.}}{18}{figure.caption.24}\protected@file@percent }
\newlabel{fig:aplanado_imagen_red_neuronal}{{2.10}{18}{Aplanado de una imagen para servir de entrada a una red neuronal}{figure.caption.24}{}}
\citation{zhang2023dive}
\citation{zhang2023dive}
\citation{dl_python__chollet_2021,nn_dl__michael_nielsen_2015,dl__goodfellow_2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Redes Neuronales Convolucionales}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:red_neuronal_convolucional}{{3}{19}{Redes Neuronales Convolucionales}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Arquitectura LeNet-5 \cite  {zhang2023dive}.}}{19}{figure.caption.25}\protected@file@percent }
\newlabel{img:lenet-5}{{3.1}{19}{Arquitectura LeNet-5 \cite {zhang2023dive}}{figure.caption.25}{}}
\citation{lecun-98}
\citation{lecun-98}
\citation{lecun-98}
\citation{Krizhevsky2012ImageNetCW}
\citation{Krizhevsky2012ImageNetCW}
\citation{imagenet-web}
\citation{Krizhevsky2012ImageNetCW}
\citation{nn_fundamentals__thakur_2021,rna_fundamentos__hilera_2021,conv_nn_face_recog__chowanda_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Arquitectura AlexNet \cite  {Krizhevsky2012ImageNetCW}.}}{20}{figure.caption.26}\protected@file@percent }
\newlabel{img:alexnet}{{3.2}{20}{Arquitectura AlexNet \cite {Krizhevsky2012ImageNetCW}}{figure.caption.26}{}}
\citation{dl__goodfellow_2016}
\citation{nn_dl__michael_nielsen_2015}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Arquitectura de una Red Neuronal Convolucional}{21}{section.3.1}\protected@file@percent }
\newlabel{sec:arquitectura_cnn}{{3.1}{21}{Arquitectura de una Red Neuronal Convolucional}{section.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Capa convolucional}{21}{section*.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Convolución discreta entre $f = [0,0,1,1,0,0]$ y filtro de detección de bordes $g = [-1,0,1]$.}}{21}{figure.caption.28}\protected@file@percent }
\newlabel{fig:convolucion_f_g_discreta}{{3.3}{21}{Convolución discreta entre $f = [0,0,1,1,0,0]$ y filtro de detección de bordes $g = [-1,0,1]$}{figure.caption.28}{}}
\citation{dl_fundamentos__casas_roma_2020,dl__goodfellow_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Convolución con filtro de Sobel en ejes ${x,y}$ para detectar bordes.}}{22}{figure.caption.29}\protected@file@percent }
\newlabel{fig:convolucion_sobel}{{3.4}{22}{Convolución con filtro de Sobel en ejes ${x,y}$ para detectar bordes}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Salida de una capa convolucional con $32$ filtros de $3\times 3$.}}{22}{figure.caption.30}\protected@file@percent }
\newlabel{fig:conv1_salidas}{{3.5}{22}{Salida de una capa convolucional con $32$ filtros de $3\times 3$}{figure.caption.30}{}}
\citation{dl__goodfellow_2016}
\citation{nn_dl__michael_nielsen_2015}
\@writefile{toc}{\contentsline {paragraph}{Capa de activación}{23}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Comparación entre una red lineal y una con activación ReLU. La introducción de no linealidad permite modelar funciones más complejas.}}{23}{figure.caption.32}\protected@file@percent }
\newlabel{fig:relu_network}{{3.6}{23}{Comparación entre una red lineal y una con activación ReLU. La introducción de no linealidad permite modelar funciones más complejas}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {paragraph}{Capa de pooling}{23}{section*.33}\protected@file@percent }
\citation{dl__goodfellow_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Operación de Max Pooling con una ventana de $2\times 2$.}}{24}{figure.caption.34}\protected@file@percent }
\newlabel{fig:pooling_2d}{{3.7}{24}{Operación de Max Pooling con una ventana de $2\times 2$}{figure.caption.34}{}}
\@writefile{toc}{\contentsline {paragraph}{Capa completamente conexa}{24}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Funcionamiento Interno: Filtrado, Stride y Padding}{24}{section.3.2}\protected@file@percent }
\newlabel{sec:funcionamiento_interno_cnn}{{3.2}{24}{Funcionamiento Interno: Filtrado, Stride y Padding}{section.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Dimensiones del filtro}{25}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stride}{25}{section*.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Ejemplo de average pooling con stride $= 1$ y stride $= 2$.}}{25}{figure.caption.38}\protected@file@percent }
\newlabel{fig:stride_avg_pooling}{{3.8}{25}{Ejemplo de average pooling con stride $= 1$ y stride $= 2$}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {paragraph}{Padding}{25}{section*.39}\protected@file@percent }
\citation{dl_python__chollet_2021,dl_fundamentos__casas_roma_2020,dl__goodfellow_2016}
\citation{dl_python__chollet_2021}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Ejemplo de average pooling con $\textbf  {padding}=1$.}}{26}{figure.caption.40}\protected@file@percent }
\newlabel{fig:padding_avg_pooling}{{3.9}{26}{Ejemplo de average pooling con $\textbf {padding}=1$}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Aprendizaje y Optimización de Filtros}{26}{section.3.3}\protected@file@percent }
\newlabel{sec:ajuste_parametros_cnn}{{3.3}{26}{Aprendizaje y Optimización de Filtros}{section.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Retropropagación en las capas convolucionales}{26}{section*.41}\protected@file@percent }
\citation{dl_python__chollet_2021}
\@writefile{toc}{\contentsline {paragraph}{Optimización}{27}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Visualización de activaciones}{27}{subsection.3.3.1}\protected@file@percent }
\citation{dl_python__chollet_2021,dl__goodfellow_2016}
\citation{nn_dl__michael_nielsen_2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Herramientas de Visualización}{28}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:herramientas_visualizacion}{{4}{28}{Herramientas de Visualización}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Mapas de Características y Activaciones}{28}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Mapa de características obtenido al pasar el kernel K2 de una capa convolucional por una imagen de entrada.}}{28}{figure.caption.43}\protected@file@percent }
\newlabel{fig:mapa_caracterisitcas_kernel_k2}{{4.1}{28}{Mapa de características obtenido al pasar el kernel K2 de una capa convolucional por una imagen de entrada}{figure.caption.43}{}}
\citation{nn_dl__michael_nielsen_2015}
\citation{visualizing__simonyan__2014}
\@writefile{toc}{\contentsline {paragraph}{Activaciones Neuronales}{29}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Métodos Basados en el Gradiente}{29}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mapas de Saliencia (Saliency Maps)}{29}{section*.45}\protected@file@percent }
\citation{visualizing__simonyan__2014}
\citation{visualizing__simonyan__2014}
\citation{visualizing__simonyan__2014}
\citation{Selvaraju17-gradcam}
\citation{examples_nn__kulik_2009,conv_nn_face_recog__chowanda_2019,slam_vehicles__saleem_2023}
\newlabel{fig:mapa_saliencia}{{\caption@xref {fig:mapa_saliencia}{ on input line 46}}{30}{Mapas de Saliencia (Saliency Maps)}{figure.caption.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Grad-CAM (Gradient-weighted Class Activation Mapping)}{30}{section*.47}\protected@file@percent }
\citation{yolo_docs__2024}
\citation{pytorch-web}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimentación}{31}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap:experimentacion}{{5}{31}{Experimentación}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Conjunto de datos: MNIST}{31}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Captura de ejemplos aleatorios del conjunto de entrenamiento de MNIST.}}{31}{figure.caption.48}\protected@file@percent }
\newlabel{fig:mnist_examples}{{5.1}{31}{Captura de ejemplos aleatorios del conjunto de entrenamiento de MNIST}{figure.caption.48}{}}
\newlabel{code:descargar_mnist}{{5.1}{31}{Descarga del dataset MNIST}{lstlisting.5.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}{\ignorespaces Descarga del dataset MNIST.}}{31}{lstlisting.5.1}\protected@file@percent }
\newlabel{line:train_dataset}{{4}{31}{Descarga del dataset MNIST}{lstnumber.5.1.4}{}}
\newlabel{line:test_dataset}{{5}{31}{Descarga del dataset MNIST}{lstnumber.5.1.5}{}}
\citation{dl_fundamentos__casas_roma_2020}
\newlabel{code:segmentar_dataset_mnist}{{5.2}{32}{Segmentación del dataset MNIST a conjunto de entrenamiento, validación y pruebas}{lstlisting.5.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}{\ignorespaces Segmentación del dataset MNIST a conjunto de entrenamiento, validación y pruebas.}}{32}{lstlisting.5.2}\protected@file@percent }
\newlabel{line:split}{{3}{32}{Segmentación del dataset MNIST a conjunto de entrenamiento, validación y pruebas}{lstnumber.5.2.3}{}}
\newlabel{line:train_loader}{{6}{32}{Segmentación del dataset MNIST a conjunto de entrenamiento, validación y pruebas}{lstnumber.5.2.6}{}}
\newlabel{line:val_loader}{{7}{32}{Segmentación del dataset MNIST a conjunto de entrenamiento, validación y pruebas}{lstnumber.5.2.7}{}}
\newlabel{line:test_loader}{{8}{32}{Segmentación del dataset MNIST a conjunto de entrenamiento, validación y pruebas}{lstnumber.5.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Distribución de muestras de MNIST en los conjuntos de entrenamiento, validación y pruebas}}{32}{figure.caption.49}\protected@file@percent }
\newlabel{fig:mnist_distribucion_datasets}{{5.2}{32}{Distribución de muestras de MNIST en los conjuntos de entrenamiento, validación y pruebas}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Métricas de Evaluación}{32}{section.5.2}\protected@file@percent }
\citation{dl_fundamentos__casas_roma_2020,dl__goodfellow_2016}
\newlabel{eq:exactitud}{{5.1}{33}{Métricas de Evaluación}{equation.5.1}{}}
\newlabel{eq:precision}{{5.2}{33}{Métricas de Evaluación}{equation.5.2}{}}
\newlabel{eq:sensibilidad}{{5.3}{33}{Métricas de Evaluación}{equation.5.3}{}}
\newlabel{eq:perdida_entropia_cruzada}{{5.4}{33}{Métricas de Evaluación}{equation.5.4}{}}
\newlabel{eq:f1_score}{{5.5}{33}{Métricas de Evaluación}{equation.5.5}{}}
\newlabel{sec:tiempo_inferencia_muestra}{{5.2}{33}{Métricas de Evaluación}{equation.5.5}{}}
\citation{googledev-rocauc}
\citation{googledev-rocauc}
\citation{googledev-rocauc}
\citation{dl_python__chollet_2021}
\newlabel{sec:roc-auc}{{5.2}{34}{Métricas de Evaluación}{equation.5.5}{}}
\newlabel{eq:tasa_falsos_positivos}{{5.6}{34}{Métricas de Evaluación}{equation.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparación de áreas bajo la cura de dos modelos de predicción \cite  {googledev-rocauc}.}}{34}{figure.caption.50}\protected@file@percent }
\newlabel{fig:auc_65_google}{{5.3}{34}{Comparación de áreas bajo la cura de dos modelos de predicción \cite {googledev-rocauc}}{figure.caption.50}{}}
\newlabel{eq:mcnemar}{{5.7}{35}{}{equation.5.7}{}}
\newlabel{eq:topk}{{5.8}{35}{}{equation.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Implementando una Red Neuronal para Clasificar Números}{35}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Implementando una Red Convolucional para Clasificar Números}{36}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Discusión}{36}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusiones y Futuras Decisiones}{37}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{nn_dl__michael_nielsen_2015}
\@writefile{toc}{\contentsline {chapter}{Ap\'{e}ndice \numberline {A}Desarrollo Matemático de la Retropropagación}{38}{appendix.1.Alph1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{anexo:retropropagacion}{{A}{38}{Desarrollo Matemático de la Retropropagación}{appendix.1.Alph1}{}}
\newlabel{f_bp:combinacion}{{A.1}{38}{Desarrollo Matemático de la Retropropagación}{equation.1.Alph1.1}{}}
\newlabel{f_bp:activacion}{{A.2}{38}{Desarrollo Matemático de la Retropropagación}{equation.1.Alph1.2}{}}
\newlabel{f_bp:combinacion_mat}{{A.3}{38}{Desarrollo Matemático de la Retropropagación}{equation.1.Alph1.3}{}}
\newlabel{f_bp:activacion_mat}{{A.4}{38}{Desarrollo Matemático de la Retropropagación}{equation.1.Alph1.4}{}}
\citation{nn_dl__michael_nielsen_2015}
\citation{nn_dl__michael_nielsen_2015}
\@writefile{toc}{\contentsline {subsubsection}{Ecuaciones detrás de la retropropagación}{39}{subsubsection*.51}\protected@file@percent }
\newlabel{f_bp:error}{{A.5}{39}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.5}{}}
\newlabel{f_bp:error_ultima_capa}{{A.7}{39}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.7}{}}
\newlabel{f_bp:error_ultima_capa_mat}{{A.8}{40}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.8}{}}
\newlabel{f_bp:error_intermedio}{{A.12}{40}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.12}{}}
\newlabel{f_bp:error_intermedio_mat}{{A.13}{40}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.13}{}}
\citation{nn_dl__michael_nielsen_2015}
\citation{dl__goodfellow_2016}
\newlabel{f_bp:ratio_sesgo}{{A.15}{41}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.15}{}}
\newlabel{f_bp:ratio_sesgo_mat}{{A.16}{41}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.16}{}}
\newlabel{f_bp:ratio_peso}{{A.17}{41}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.17}{}}
\newlabel{f_bp:ratio_peso_mat}{{A.18}{41}{Ecuaciones detrás de la retropropagación}{equation.1.Alph1.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{El algoritmo de retropropagación}{41}{subsubsection*.52}\protected@file@percent }
\bibstyle{unsrtnat}
\bibdata{bibliografia.bib}
\newlabel{f_bp:regla_delta}{{A.19}{42}{El algoritmo de retropropagación}{equation.1.Alph1.19}{}}
\bibcite{zhang2023dive}{{1}{2023}{{Zhang et~al.}}{{Zhang, Lipton, Li, and Smola}}}
\bibcite{Krizhevsky2012ImageNetCW}{{2}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{googledev-rocauc}{{3}{}{{goo}}{{}}}
\bibcite{examples_nn__kulik_2009}{{4}{2009}{{Kulik and Nikonets}}{{}}}
\bibcite{nn_dna_sequences__snyder_1993}{{5}{1993}{{Snyder and Stormo}}{{}}}
\bibcite{machine_translation__mahata_2019}{{6}{2019}{{Mahata et~al.}}{{Mahata, Das, and Bandyopadhyay}}}
\bibcite{chatbot_customer_service__nuruzzaman_2018}{{7}{2018}{{Nuruzzaman and Hussain}}{{}}}
\bibcite{conv_nn_face_recog__chowanda_2019}{{8}{2019}{{Chowanda and Sutoyo}}{{}}}
\bibcite{slam_vehicles__saleem_2023}{{9}{2023}{{Saleem et~al.}}{{Saleem, Malekian, and Munir}}}
\bibcite{rna_fundamentos__hilera_2021}{{10}{1995}{{Gonz{\'a}lez and Hernando}}{{}}}
\bibcite{nn_fundamentals__thakur_2021}{{11}{2021}{{Thakur and Konde}}{{}}}
\bibcite{yolo_docs__2024}{{12}{2024}{{Inc.}}{{}}}
\bibcite{neurocomputing__hecht_nielsen_1998}{{13}{1988}{{Hecht-Nielsen}}{{}}}
\bibcite{intro_rna__rivera_2005}{{14}{2005}{{Rivera}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliograf\'{\i }a}{43}{appendix*.53}\protected@file@percent }
\bibcite{dl_python__chollet_2021}{{15}{2021}{{Chollet and Chollet}}{{}}}
\bibcite{analisis_matematico__gordon_1995}{{16}{1986}{{Gordon et~al.}}{{Gordon, Boti, and Mar{\'\i }n}}}
\bibcite{dl_fundamentos__casas_roma_2020}{{17}{2020}{{Casas Roma, J.}}{{}}}
\bibcite{nn_dl__michael_nielsen_2015}{{18}{2015}{{Nielsen}}{{}}}
\bibcite{perceptron__rosenblatt_1958}{{19}{1958}{{Rosenblatt}}{{}}}
\bibcite{dl__goodfellow_2016}{{20}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{lecun-98}{{21}{1998}{{Lecun et~al.}}{{Lecun, Bottou, Bengio, and Haffner}}}
\bibcite{imagenet-web}{{22}{}{{ima}}{{}}}
\bibcite{visualizing__simonyan__2014}{{23}{2014}{{Simonyan et~al.}}{{Simonyan, Vedaldi, and Zisserman}}}
\bibcite{Selvaraju17-gradcam}{{24}{2017}{{Selvaraju et~al.}}{{Selvaraju, Cogswell, Das, Vedantam, Parikh, and Batra}}}
\bibcite{pytorch-web}{{25}{}{{Contributors}}{{}}}
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{16.96295pt}
\gdef \@abspage@last{50}
